<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/03/16/BN%20and%20GN/"/>
    <url>/2021/03/16/BN%20and%20GN/</url>
    
    <content type="html"><![CDATA[<p>为了追求更高的性能，卷积网络被设计的越来越深，然而网络却变得难以训练收敛与调参。</p><p>原因在于，浅层参数的微弱变化经过多层线性变换和激活函数后会被放大，改变了每一层的输入分布，造成深层的网络需要不断调整以适应这些分布变化，最终导致模型难以训练收敛。</p><p>这种网络中参数变化导致的内部节点数据分布发生变化的现象称为 <code>ICS</code>。<code>ICS</code> 现象容易使训练过程陷入饱和区，减慢模型收敛。</p><p><code>ReLU</code> 函数从激活函数角度，在一定程度上解决了梯度饱和的现象。</p><p><code>BN</code> 层从<strong>改变数据分布的角度</strong>避免了参数陷入饱和区，当前已经成为卷积网络的标配。</p><p><code>BN</code> 层首先对每一个 <code>batch</code> 的输入特征进行白化操作，即去均值方差过程。假设一个 <code>batch</code> 的输入数据为 $x:B = {x_1,…,x_m}$，求得该 <code>batch</code> 数据的均值与方差：<br>$$<br>\mu_B \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i<br>$$</p><p>$$<br>\sigma_B^2 \leftarrow \frac{1}{m}\sum_{i=1}^m(x_i-\mu_B)^2<br>$$</p><p>以上公式中，m 代表 <code>batch</code>  的大小，$\mu_B$ 为批处理数据的均值，$\sigma_B^2$ 为批处理数据的方差。在求得均值方差后，利用公式（3）进行去均值方差操作。<br>$$<br>\widehat{x}_i \leftarrow \frac{x_i -\mu_B}{\sqrt{\sigma_B^2 + \xi} }<br>$$<br>白化操作可以使输入的特征分布具有相同的均值与方差，固定了每一层的输入分布，从而加速网络的收敛。然而，白化操作虽然在一定程度上避免了梯度饱和，但是也限制了网络中数据的表达能力，浅层学习得到的参数信息会被白化操作给屏蔽掉。因此，<code>BN</code> 层在白化操作后新增了一个<strong>线性变换操作</strong>，让数据尽可能回复本身的表达能力。<br>$$<br>y_i \leftarrow \gamma\widehat{x}_i + \beta<br>$$<br>$\gamma$ 和 $\beta$ 是新引进的可学习参数，<code>BN</code> 层最终的输出为 $y_i$。</p><p>所以，一个<code>BN</code> 层会保留 <code>4C</code> 个参数，分别是 <code>C</code>个$\gamma$，<code>C</code>个$\beta$，<code>C</code>个$\mu_B$，<code>C</code>个$\sigma_B^2$，<code>C</code> 是通道数。</p><p><code>BN</code> 层的优势主要有三点：</p><ol><li>缓解梯度消失，加速网络收敛。<code>BN</code> 层让激活函数的输入数据落在非饱和区，缓解了梯度消失问题。此外，由于每层数据的均值与方差都在一定范围内，深层网络不必不断适应浅层网络输入的变化，实现了<strong>层间解耦</strong>，允许每一层独立学习，同样加快了网络收敛。</li><li>简化调参，网络更稳定。在调参时，学习率调得过大容易出现震荡与不收敛，<code>BN</code> 层则抑制了参数微小变化随着网络不断加深而被放大得问题，因此对于参数变化得适应能力更强，更容易调参。</li><li>防止过拟合。<code>BN</code> 层将每一个 <code>batch</code> 得均值与方差引入网络中，由于每一个 <code>batch</code> 得这两个值都不相同，可看作是为训练过程中增加了随机噪音，起到了一定的正则效果，防止过拟合。</li></ol><blockquote><p>在测试时，由于是对单样本进行测试，没有 <code>batch</code> 的均值与方差，通常做法是在训练时将每一个 <code>batch</code> 的均值与方差都保留下来，在测试时使用所有训练样本均值与方差的平均值。</p></blockquote><p><code>BN</code> 层存在的问题：</p><ol><li><p>由于是在 <code>batch</code> 的维度进行归一化，<code>BN</code> 层要求具有较大的 <code>batch</code> 才能有效地工作（<code>batch</code> 值比较小不能代表整个训练集），而物体检测、分割任务占用内存比较高，限制了 <code>batch</code> 的大小，这会限制<code>BN</code> 层的效果。</p></li><li><p>在训练时采用滑动来计算平均值与方差，在测试时直接拿训练集的平均值与方差使用，这会导致测试集过于依赖训练集。</p></li></ol><p>上面存在的问题 1 可以通过工程上的 <code>trick</code> 解决，即多训练几次在进行参数更新。</p><p>还可以使用如下的 <code>GN</code> 方法解决 <code>batch size</code> 过小的问题，这一想法来源于论文 <a href="https://arxiv.org/pdf/1803.08494.pdf">Group Normalization</a>。</p><p><code>GN</code> 方法（下图最右边）选择避开 <code>batch</code> 进行归一化，从通道方向计算均值与方差，使用更为灵活。   </p><div align=center>    <img src='https://img2020.cnblogs.com/i-beta/1456303/202003/1456303-20200321105117611-734247639.png'></div><blockquote><p><code>GN</code> 归一化的公式与 <code>BN</code> 相同，即先计算均值和方差，后进行线性变换重构，不同的是选择哪些像素集合进行归一化。</p></blockquote><p>如上图所示，蓝色表示选取哪些集合来计算均值与方差。</p><p><code>BN</code> 层沿着维度 <code>C</code> 的方向依次计算 <code>(N,H,W)</code> 的均值与方差；<code>LN</code> 层沿着维度 <code>N</code> 的方向，依次计算 <code>(C,H,W)</code> 的均值与方差；<code>IN</code> 沿着维度 <code>N</code> 和维度 <code>C</code> 的方向，依次计算 <code>(H,W)</code> 的均值与方差。</p><p>对于 <code>GN</code>，将一个样本的 <code>C</code> 个特征通道分成 <code>G</code> 组，每组包含 $\lfloor\frac{C}{G}\rfloor$ 个特征通道，我们将使用每个样本下的每个分组来计算均值和方差，即计算 $\frac{C}{G} \times H \times W$ 个值得均值和方差。在这里，<code>G</code> 是超参数，$\lfloor \cdot \rfloor$ 是代表向下取整。如上所示的 <code>GN</code> 示意图为例，<code>batch size</code> 为 <code>6</code>，每个样本有 <code>6</code> 个特征通道，我们将其分成 <code>G = 2</code> 组，此时每组具有 $C/G = 3$ 个通道，即蓝色部分，对蓝色部分计算均值和方差。</p><p>论文 <a href="https://arxiv.org/pdf/1803.08494.pdf">Group Normalization</a> 给出了 <code>GN</code> 的 Tensorflow 的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">GroupNorm</span>(<span class="hljs-params">x, gamma, beta, G, eps=<span class="hljs-number">1e-5</span></span>):</span><br>    <span class="hljs-comment"># x: input feature with shape [N,C,H,W]</span><br>    <span class="hljs-comment"># gamma,beta: scale and offset, with shape [1,C,1,1]</span><br>    <span class="hljs-comment"># G: number of groups for GN</span><br>    N, C, H, W = x.shape<br>    x = tf.reshape(x, [N, G, C//G, H, W])<br>    mean, var = tf.nn.moments(x, [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>], keep_dims=<span class="hljs-literal">True</span>)<br>    x = (x - mean) / tf.sqrt(var + eps)<br>    <br>    x = tf.reshape(x, [N, C, H, W])<br>    <br>    <span class="hljs-keyword">return</span> x * gamma + beta<br></code></pre></td></tr></table></figure><p>如果 <code>G=1</code> 的话，即使用所有特征通道，等于变成了 <code>LN</code>，若 <code>G=C</code> 的话，即只使用一个通道，等于变成了<code> IN</code>。 </p><div align=center>    <img src='https://img2020.cnblogs.com/i-beta/1456303/202003/1456303-20200321120116153-1366143415.png'></div><p>从论文实验结果来看，当 <code>batch size</code> 为 <code>32</code> 时，验证集上 <code>BN</code> 的效果略好于 <code>GN</code> 层，两者的差异不大，但当 <code>batch size</code> 变小时，使用 <code>BN</code> 层的模型性能迅速下降，而 <code>GN</code> 层不怎么受 <code>batch size</code> 的影响，表现比较稳定。</p><blockquote><p>作者提出，在目标检测或者语义分割时，头结构使用 <code>GN</code> 的效果比使用 <code>BN</code> 的效果要好！</p><p>原因是，在目标检测中，<code>ROI</code> 区域是从相同的图片采样得到的，它们之间并不满足独立同分布，而非独立同分布会弱化 <code>BN</code> 层的均值和方差分布，所以导致头结构中使用 <code>BN</code> 层效果更差。</p></blockquote><blockquote><p>一种关于 <code>GN</code> 层效果的猜想，即不同的通道代表了不同的意义，例如形状、边缘、纹理等，这些通道并不是完全独立同分布的，而是可以放到一起进行归一化分析。</p></blockquote><p><code>GN</code> 解决了 <code>batch size</code> 的问题，但在正常的 <code>batch size</code> 时，其精度依然比不上 <code>BN</code> 层。</p><div align=center>    <img src='https://img2020.cnblogs.com/i-beta/1456303/202003/1456303-20200321171142513-289115620.png'></div><p>2019年谷歌在论文<a href="">Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks</a>提出 <code>FRN</code> 层的概念，解决了归一化不依赖 <code>batch size</code>，又能使精度高于 <code>BN</code> 层。</p><p><code>FRN</code> 层由归一化层 <code>FRN</code>（Filter Response Normalization）和激活层 <code>TLU</code> (Thresholded Linear Unit)组成，如下图，</p><div align=center>    <img src='https://img2020.cnblogs.com/i-beta/1456303/202003/1456303-20200321171537285-644511696.png'></div><p><code>FRN</code> 是在 <code>(H,W)</code> 的维度上进行，即对每一个样例的每一个 <code>channel</code> 单独进行归一化，这里的 <code>x</code> 是维度为 <code>N = H*W</code> 的向量，即使用 <code>N</code> 个特征值来求取平均的平方和 $v^2$。</p><p>注意，<code>FRN</code> 没有减去均值的操作，所以导致归一化后特征值不是关于 0 对称的，会以任意的方式偏移 0 值。假设后面以 <code>ReLU</code> 为激活函数的话，可能会产生很多 0 值，产生误差。因此，需要对 <code>ReLU</code> 进行增强，即 <code>TLU</code>。通过引入一个可学习的阈值 $\tau$。<br>$$<br>z_{TLU} = max(y,\tau) = max(y - \tau,0) + \tau = ReLU(y - \tau) + \tau<br>$$<br>所以，<code>FRN</code> 层引入了 $\gamma、\beta、\tau$ 三个可学习的参数，分别表示变换重构的尺度、偏移和阈值，他们都有 <code>C</code> 个值，分别对应着每一个通道。</p><p>在 <code>N = H*W = 1</code> 的情况下（即特征图大小为 $1\times1$ ），如果 $\varepsilon$ 取值很小，则归一化函数会近似于符号函数，梯度值近似为 0，不利于优化；若 $\varepsilon$ 相对较大，则曲线会变得更加平滑，此时梯度有利于模型学习。</p><div align=center>    <img src = 'https://img2020.cnblogs.com/i-beta/1456303/202003/1456303-20200321175624305-576530170.png'></div><p>因此，在 $N = 1$ 的情况下，将 $\varepsilon$ 变成一个可学习的参数（初始化为 10^-4^）；对于 $N \neq 1$ 的情况下，将 $\varepsilon$ 固定成 10^-6^。为了保证可学习参数 $\varepsilon &gt; 0$，对其进行一定限制， $\varepsilon = 10^{-6} + |\varepsilon_l|$。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Residual Networks</title>
    <link href="/2021/03/16/Residual-Networks/"/>
    <url>/2021/03/16/Residual-Networks/</url>
    
    <content type="html"><![CDATA[<h3 id="Residual-Block"><a href="#Residual-Block" class="headerlink" title="Residual Block"></a>Residual Block</h3><p>残差 residual 这一思想起源于论文<a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>。该论文发现，如果存在某个 <strong>K</strong> 层的网络 <strong>f</strong> 是当前最优网络，那么可以构造一个更深的网络，其最后几层是网络 <strong>f</strong> 第 <strong>K</strong> 层输出的恒等映射（Identity Mapping），可以取得与网络 <strong>f</strong> 一致的效果。如果 <strong>K</strong> 不是已有网络的最佳层数，那么新构造的网络应该可以取得更好的效果。但是，实验结果却正好与此相反。</p><div align=center>    <img src='https://pic2.zhimg.com/80/v2-dcf5688dad675cbe8fb8be243af5e1fd_720w.jpg'></div><p>随着网络的加深，反而出现了训练集准确率下降的问题，即深层网络的效果反而不如浅层网络。造成这种现象的原因究竟是什么呢？</p><p>一般来讲，模型更复杂，其效果更好，也更容易发生过拟合现象。</p><ol><li>根据上述结果可以排除过拟合的可能，因为过拟合在训练过程中会发生 loss 持续减小，准确率一直增加的现象。（即若是过拟合，红线应该在黄线下面。）</li><li>在网络中采用 <code>Batch Normalization</code> 方法也可以排除梯度消失和梯度爆炸的问题。</li></ol><p>把这个新发现的问题定义为网络的退化（<code>degradation</code>），即随着网络层数的增加，网络效果却急剧下降的情况。</p><p>浅层网络的解空间是包含在深层网络的解空间中的，即深层网络的效果至少不会比浅层网络差。但大量实验结果表明，深层网络的效果往往不如浅层网络。更好的解明明存在，但找到的往往是效果不那么好的解，这个问题该如何解决？</p><p>这显然是一个优化问题！上述现象还反映出，结构相似的模型优化难度是不一样的，模型越深优化的难度越高。</p><p>论文作者从调整模型结构的角度入手来考虑优化模型，即引入残差块（<code>Residual Block</code>）。</p><p>残差块表示如下：</p><div align=center>    <img src='https://s2.ax1x.com/2020/02/21/3uUio4.png'></div><p>一个残差块有 2 条路径  $\mathcal{F(X)}$ 和 $\mathcal{X}$，$\mathcal{F(X)}$ 路径拟合残差，$\mathcal{X}$ 为 <code>identity mapping</code> 的恒等映射，称之为 <code>shortcut</code>。</p><p> 图中的  $\oplus$ 为 <code>element-wise addition</code>，要求参与运算的 $\mathcal{F(X)}$ 和  $\mathcal{X}$ 的 <code>shape</code> 要相同。</p><ol><li>残差路径如何设计？</li><li><code>shortcut</code> 路径如何设计？</li><li><code>Residual Block</code> 之间如何连接？</li></ol><p>在论文中，残差路径大致可以分为 2 种，一种是 <code>bottleneck</code> 结构，即右下图中的 1$\times$1卷积层，先降维再升维，主要用于降低计算复杂度的考虑，将此种残差块称为 <code>bottleneck bock</code>。另一种结构如左下图所示，称之为 <code>basic block</code>，其由 2 个 3$\times$3 的卷积层构成。</p><div align=center>    <img src='https://s2.ax1x.com/2020/02/21/3K34c8.png'></div><p><code>shortcut</code> 路径也分为 2 种，取决于残差路径是否改变了 <code>feature map</code>的数量和尺寸（即宽高和深度信息）。 一种是将输入 $\mathcal{X}$ 原封不动地输出，另一种将 $\mathcal{X}$ 经过 1$\times$1 卷积来升维/降维，目的就是为了保持与残差路径的输出 <code>shape</code> 保持一致以顺利进行 $\oplus$ 操作，其对网络性能的提升并不明显。</p><blockquote><p>需要注意，有 1$\times$1 卷积的 <code>shortcut</code> 路径在更新参数时，其路径的卷积权重参数也是要更新的。</p></blockquote><div align = center>    <img src = 'https://s2.ax1x.com/2020/02/23/3l4cD0.png'></div><p>至于 <code>Residual Block</code>之间的衔接，在原论文中，$\mathcal{F(X)}$ $+$ $\mathcal{X}$  经过 <code>ReLU</code> 后直接作为下一个 <code>Residual Block</code>的输入 $\mathcal{X}$ 。</p><h3 id="ResNet-网络结构"><a href="#ResNet-网络结构" class="headerlink" title="ResNet 网络结构"></a>ResNet 网络结构</h3><p><code>ResNet</code> 是多个 <code>Residual Block</code> 的串联，下面直观感受下 <code>ResNet-34</code> 与 <code>34-layer plain net</code> 和 <code>VGG</code> 的对比。</p><div align=center>    <img src='https://s2.ax1x.com/2020/02/21/3u8Wwj.png'></div><p><code>ResNet</code> 网络有如下几个特点：</p><ul><li>与 <code>plain net</code> 相比，<code>ResNet</code> 多了很多<strong>旁路</strong>，即 <code>shortcut</code> 路径。每一个 <code>shortcut</code> 首位圈出的 <code>layers</code> 构成一个 <code>Residual Block</code>。</li><li><code>ResNet</code> 中，所有的 <code>Residual Block</code> 都没有 <code>pooling</code> 层，降采样是通过 <code>conv</code> 的 <code>stride</code> 实现的。</li><li>分别在 <code>conv3_1</code>、<code>conv4_1</code>、<code>conv5_1</code> 的 <code>Residual Block</code>，降采样一倍，同时 <code>feature map</code> 的数量增加一倍，如上图虚线划定的 <code>block</code>。</li><li>通过 <code>Average Pooling</code> 得到最终的特征，而不是通过 <code>FC</code>。</li><li>每个卷积层后都跟有 <code>BatchNorm layer</code>，为了简化示意图，并没有标出。</li></ul><p><code>ResNet</code>结构非常容易修改和扩展，通过调整 <code>block</code> 内的 <code>channel</code> 数量以及堆叠的 <code>block</code> 数量，就可以很容易地调整网络的宽度和深度，来得到不同表达能力的网络，而不用过多地担心网络的 <code>degradation</code> 问题。通常情况下，只要训练足够，逐步加深网络，就可以获得更好的性能表现。</p><p>下图为网络性能的对比：</p><div align=center>    <img src='https://s2.ax1x.com/2020/02/24/38EfB9.png'></div><p>上面的实验说明，不断地增加 <code>ResNet</code> 的深度，甚至增加到 1000 层以上，网络也没有发生退化现象。由此说明 <code>Residual Block</code> 的有效性。</p><p><code>ResNet</code> 的思想在于认为拟合残差比直接拟合潜在映射更容易优化 。</p><p>可以通过绘制 <code>error surface</code> 来理解为什么网络加入 <code>shortcut</code> 之后更容易优化。</p><div align=center>    <img src='https://s2.ax1x.com/2020/02/24/38Kd2t.png'></div><p>可以发现：</p><ol><li><code>ResNet-20 (no short)</code>浅层 <code>plain net</code> 的 <code>error surface</code> 还没有很复杂，优化也会很困难，当网络增加到 56 层后，网络优化难度急剧上升。对于 <code>plain net</code> ，随着深度增加，<code>error surface</code> 迅速恶化。</li><li>引入 <code>shortcut</code> 后，<code>error surface</code> 变得平滑了一些，梯度的可预测性变得更好，优化也变得更容易。</li></ol><h3 id="Residual-Block-的分析与改进"><a href="#Residual-Block-的分析与改进" class="headerlink" title="Residual Block 的分析与改进"></a>Residual Block 的分析与改进</h3><p>论文 <code>Identity Mappings in Deep Residual Networks</code> 进一步研究 <code>ResNet</code>，通过 <code>ResNet</code>反向传播的理论分析及调整 <code>Residual Block</code> 的结构，得到了新的  <code>Residual Block</code> 形式，如下：</p><div align=center>    <img src='https://s2.ax1x.com/2020/02/24/3G0uVJ.png'></div><blockquote><p>注意，这里将 <code>shortcut</code> 视为主干路径，将残差路径视为旁路。</p></blockquote><p>新提出的 <code>Residual Block</code> 结构具有更强的泛化能力，能够更好的避免退化。新结构的具体变化在于：</p><ol><li>保持 <code>shortcut</code> 路径的<strong>纯净</strong>，可以让信息在前向传播和反向传播中平滑传递。如无必要，不引入 1$\times$1 卷积，同时将上图灰色路径上的 <code>ReLU</code> 移到 $\mathcal{F(X)}$ 路径上。</li><li>在残差路径上，将 <code>BN</code> 和 <code>ReLU</code> 放在 <code>weight</code> 前作为 <code>pre-activation</code>，获得 <code>Ease of optimization</code> 以及 <code>Reducing overfitting</code> 的效果。</li></ol><p>下面具体解释一下！</p><p>令 <code>h</code> 为 <code>shortcut</code> 路径上的变换，<code>f</code>为 <code>addition</code> 之后的变换，原 <code>Residual Block</code> 中 <code>f = ReLU</code> 当 <code>h</code> 和 <code>f</code> 均为 <code>identity mapping</code> 时，可以得到任意两层 $\mathcal{X}$<del>L</del> 和 $\mathcal{X}$<del>l</del> 之间的关系，此时信息可以在 $\mathcal{X}$<del>L</del> 和 $\mathcal{X}$<del>l</del> 间无损直达。</p><p>给出如下公式：<br>$$<br>y_l = h(X_l) + \mathcal{F(X_l,W_l)}  \<br>X_{l+1} = \mathcal{f(y_l)}  \<br>X_{l+1} = X_l + \mathcal{F(x_l,W_l)}\<br>X_L = X_l + \sum_{i=1}^{L-1}\mathcal{F(X_i,W_i)}   \<br>$$<br>对于前向传播来说，分析公式可以得到：</p><ol><li>第 $\normalsize{L}$ 层的特征 $X_L$ 可以分为两个部分，第一部分是浅层网络表示 $X_l$，第二部分是残差函数映射 $\sum_{i=l}^{L-1}\mathcal{F(X_i,W_i)}$ ，表明模型在任意单元内都是一个残差形式。</li><li>对于任意深度 $L$ 的特征 $X_L$ 来讲，它是前面所有残差模块的和，这与简单的不加 <code>shortcut</code> 网络完全相反。原因是，不加 <code>shortcut</code> 的网络在第 $L$ 层的特征 $X_L$ 是一系列向量乘的结果，即 $\prod_{i=0}^{L-1}W_{i}X_0$。（在忽略 <code>Batch Normalization</code> 和 <code>ReLU</code> 的情况下）</li></ol><p>基于上面第二点，我们发现这也有非常好的反向传播特性。假设损失为 $\Large\epsilon$，根据链式求导法则，得到如下结果：<br>$$<br>\frac{\partial\LARGE\epsilon}{\partial X_l} = \frac{\partial\LARGE\epsilon}{\partial X_L}\frac{\partial X_L}{\partial X_l} = \frac{\partial\LARGE\epsilon}{\partial X_L}\LARGE{(} \normalsize{1 + \frac{\partial}{\partial X_L}}\sum_{i=1}^{L-1}\mathcal{F(X_i,W_i)}\LARGE{)}  \<br>$$<br>即梯度 $\LARGE \frac{\partial\epsilon}{\partial X_l}$ 由两个部分组成，一部分 $\LARGE\frac{\partial \epsilon}{\partial X_L}$ 是不经过任何权重加权的信息流，另一部分是通过权重加权层的 $\LARGE\frac{\partial\Large\epsilon}{\partial X_L}\LARGE{(} \normalsize{1 + \frac{\partial}{\partial X_L}}\normalsize\sum_{i=1}^{L-1}\mathcal{F(X_i,W_i)}\LARGE{)}$，两部分的线性特性保证了信息可以直接反向传播到浅层。</p><p>同时，反向传播中的这个 <code>1</code> 具有非常好的性质！即任意两层之间的反向传播，这一项都是 <code>1</code>，可以有效避免梯度消失和梯度爆炸。（此基于 <code>h</code> 和 <code>f</code> 均为 <code>identity mapping</code> ）</p><p>如果 <code>h</code> 和 <code>f</code> 不是 <code>identity mapping</code> ，就会使得反向传播公式变得复杂，有可能导致梯度消失或爆炸，且层数越多越明显。</p><blockquote><p>值得注意的是，<code>BN</code> 层解决了 <code>plain net</code> 的梯度消失和爆炸问题，这里的 <code>1</code> 可以避免 <code>shortcut</code> 路径上的梯度消失和爆炸问题。</p></blockquote><p> <code>shortcut</code> 路径将反向传播由连乘形式变为加法形式，让网络最终的损失在反向传播时可以无损直达每一个 <code>block</code>，意味着每一个 <code>block</code> 的权重更新部分都直接作用在最终的损失上。观察前面前向传播的公式，发现虽然信息在任意两层间直达，但这种直达是隐含的，只能看到加法的结果，而无法清楚每个加数是多少。从信息通路上来讲，这是不彻底的，由此也推动了 <code>DenseNet</code>的诞生。</p><p>对于残差路径的改进，作者进行了一系列对比试验，最终得出将 <code>BN</code> 和 <code>ReLU</code> 放在 <code>weight</code> 之前有利于提高模型的精度，将此命名为 <code>full preactivation</code> 结构。</p><div align=center>    <img src='https://s2.ax1x.com/2020/02/24/3GcfnP.png'></div><p><code>ResNet</code> 的目的是为了解决网络退化问题。<code>Residual Block</code>的设计让学习 <code>identity mapping</code> 更容易，即使堆叠了过量的 <code>block</code>， 也可以让冗余的 <code>block</code> 学习成恒等映射，性能不会下降。即网络的深度是在训练过程中决定的，<code>ResNet</code> 具有某种深度自适应的能力。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/01/10/hello-world/"/>
    <url>/2021/01/10/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
